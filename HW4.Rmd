---
title: 'HW4: Team 12'
author: "Thomas Fleming, Blaire Li, Marc Ryser, Hengqian Zhang"
date: "Due February 17, 2016"
output:
  pdf_document: default
  html_notebook: default
---


```{r setup, echo=FALSE}
suppressMessages(library(ISLR))
suppressMessages(library(arm))
suppressMessages(library(ggplot2))
suppressMessages(library(dplyr))
suppressMessages(library(GGally))
suppressMessages(library(tidyr))
library(knitr)
# post on piazza for additional packages if there are wercker build errors due to missing packages
```

Load the college application data from Lab1 and create the variable `Elite` by binning the `Top10perc` variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50 %.  We will also save the College names as a new variable and remove `Accept` and `Enroll` as temporally they occur after applying, and do not make sense as predictors in future data.

```{r data}
data(College)

College = College %>% mutate(college = rownames(College)) %>%
                      mutate(Elite = factor(Top10perc > 50)) %>%
#                      mutate(Elite = 
 #                              car::recode(Elite, "TRUE" = "Yes", "FALSE" ="No")) #%>%
                      dplyr::select(c(-Accept,-Enroll))
                      


colnames(College)
```

We are going to create a training and test set by randomly splitting the data.  First set a random seed by

```{r setseed}
# do not change this; for a break google `8675309`
set.seed(8675309)
n = nrow(College)
n.train = floor(.75*n)
train = sample(1:n, size=n.train, replace=FALSE)
College.train = College[train,]
College.test = College[-train,]
```


1.  Summarize the training and test data, and comment on which variables are numeric, factors or that could be treated as either.
Comment on whether the summaries appear to be similar across the test and training data sets.

```{r}
#summary for training set
summary(College.train[,-17])

#summary for test set
summary(College.test[,-17])

```
Answer: catergorical variables: Private, Elite. College name should not be treated as a variable. All other variables left should be treated as numerical variables. 

From the summaries given above, variables in both train set and test set have similar ranges and mean. One thing we need pay attention to is that the ratio of public school vs private school in training set is 0.43 while the ratio in test set is only 0.24. This may introduce biase since private school and public school have many difference.

2. Create scatter plots of predictors versus `Apps` using the training data only.  If you use pairs or preferably `ggpairs` make sure that `Apps` is on the y-axis in plots versus the other predictors.  (Make sure that the plots are legible, which may require multiple plots.)  
Comment on any features in the plots, such as potential outliers, non-linearity, needs for transformations etc.
```{r}
#melt data
College.train_melt = College.train %>% select(-c(Private,college, Elite)) %>% 
                     gather(feature, value, c(Top10perc:Grad.Rate))

College.train_melt_f = College.train %>% select(-college) %>% 
                                         select(c(Private,Apps,Elite)) %>% 
                                         gather(feature, value, c(Private,Elite))

College.train_melt_1 = College.train_melt[1:2328,]
College.train_melt_2 = College.train_melt[2329:4656,]
College.train_melt_3 = College.train_melt[4657:6984,]
College.train_melt_4 = College.train_melt[6985:8148,]

ggplot(College.train_melt_f, aes(value, Apps)) +
  geom_point(color = "firebrick") +
  facet_wrap(~feature, scales = "free")

ggplot(College.train_melt_1, aes(value, Apps)) +
  geom_point(color = "firebrick") +
  facet_wrap(~feature, scales = "free")

ggplot(College.train_melt_2, aes(value, Apps)) +
  geom_point(color = "firebrick") +
  facet_wrap(~feature, scales = "free")

ggplot(College.train_melt_3, aes(value, Apps)) +
  geom_point(color = "firebrick") +
  facet_wrap(~feature, scales = "free")

ggplot(College.train_melt_4, aes(value, Apps)) +
  geom_point(color = "firebrick") +
  facet_wrap(~feature, scales = "free")


```


ANSWER:We first looked at two categorical variables: Elite and Private. Under Yes or No, most colleges have applications less than 15000. So, we can consider applications greater than 20000 are outliers.

We then paid attention to numerical variables. We could see clearly that the number of full time undergraduate has strong linear relation with applications. Variables 'Top10perc' and 'Top25perc' have weak linear relationship with applications. For other numberical variables, there is no linear relation with applications. Outliers exist in all variables which indicate appropriate tranformation is needed. 

3.  Build a linear regression model to predict `Apps` from the other predictors (without any transformations).  Present model summaries and diagnostic plots.   Based on diagnostic plots  using residuals,  comment on the  adequacy of your model.
```{r}

app_lm = lm(Apps ~.-college, data  = College.train)

summary(app_lm)
par(mforw = c(2,2))
plot(app_lm, labels.id = College.train$college)


```
ANSWER: From the residual plot vs fitted value, we see that the residual variance increases. The model predicts negative Apps, which is not meaningful. From the QQplot, there is clear heavy tail. From scale-location plot, we can see an increasing, linear trend of the standardized residuals vs fitted values. Therefore, we can conclude that a linear model without transformations and/or interactions is not adequate.

4. Generate 1000 replicates data sets using the coefficients from the model you fit above.  Using RMSE as a statistic, $\sqrt{\sum_i(y^{\text{rep}} - \hat{y}_i^{\text{rep}})^2/n }$, how does the RMSE from the model based on the training data compare to RMSE's based on the replicated data.  What does this suggest about model adequacy?   Provide a histogram of the RMSE's with a line showing the location of the observed RMSE and compute a p-value.  Hint:  write a function to calculate RMSE.




```{r}
#rmse function
rmse = function(y, ypred){
  rmse=sqrt(mean((y-ypred)^2))
  return(rmse)
}

#number of replicates
nsim=1000
#Create design matrix
X = model.matrix(app_lm)
#Get 1000 samples of parameters
params = sim(app_lm, nsim)
#Create matrix to store 1000 sets of y's
y_rep = matrix(0, nrow = nsim ,ncol = nrow(College.train))
#Create vector to store 1000 RMSE statistic
RMSE.2<-rep(0,nsim)
#compute RMSE based on simulations
for(i in 1:nsim){
y_rep[i,] = X %*% params@coef[i,] + rnorm(1, 0, params@sigma[i])
RMSE.2[i]<-rmse(predict(app_lm), y_rep[i,])
  
}
RMSE.2 = data.frame(RMSE.2)

#compute RMSE based on train data
RMSE.1<-rmse(College.train$Apps,predict(app_lm))

#plot the histogram
GGPLOT.4<-ggplot(data= RMSE.2, aes(x=RMSE.2))+
  geom_histogram(color = "firebrick") + 
  geom_vline(xintercept = RMSE.1, color="red")+labs(x="RMSE", y="Count")
plot(GGPLOT.4)
#compute p-value
pval<-sum(RMSE.2>RMSE.1)/nsim
print(pval)

```
ANSWER: The RMSE of the observed data is not inconsistent with the simulated model. In fact the tail probability, or p-value, of 'r pval' for RMSE is too large to not reject this model (based on this particular statistic).


5. Build a second model, considering transformations of the response and predictors, possible interactions, etc with the goal of trying to achieve  a model where assumptions for linear regression are satisfied, providing justification for your choices.
Comment on  how well the assumptions are met and and issues that diagnostic plots may reveal.


ANSWER: First we perform alog transformation on 'Apps'. From the residual plot, we can see that the latter is much better than for the non-transformed model. We also applied the boxcox procedure to find the best y tranformation. And we can see 0 is close to the best parameter. Therefore, for a better intepratation of y, we admit log transformation on y.

```{r}
#log transformation on y
app_lm_yt = lm(log(Apps)~.-college, data = College.train)
par(mfrow = c(2,2))
#diagnostic plots
plot(app_lm_yt)
#boxcox procedure to find best y transformation
boxcox(app_lm)
```

However, there is still a linear trend between residuals and fitted value based on the residual plots. We next move to x transformations. Let us go back to the scatter plots in question 2. As we have already seen, data is not evenly distributed. We applied log transformation on x which have many leverage points. For 'Top25perc' and 'Grad.Rate', we applied quadratic transformation on x since there is quadartic trend.

```{r}

#log transformation on y
app_lm_t = lm(log(Apps) ~ Private + log(Top10perc) + (Top25perc)^2 +       
                           log(F.Undergrad) +log(P.Undergrad) +  Outstate + 
                           (Room.Board) + log(Books) + log(Personal) +         
                           log(PhD)+Terminal+log(S.F.Ratio)+ perc.alumni +    
                           log(Expend) + (Grad.Rate)^2+ Elite, 
                           data=College.train)
summary(app_lm_t)
#diagnostic plots
plot(app_lm_t)
```


Finally, we applied step function(BIC) to see if there are potential interation terms. From the plots, we can see, both residual plots and QQplot are much better than before. There, we finally choose the model given below are the best linear model for this data. 
```{r}
# Systematic check for interactions
app_lm_t2<-lm(log(Apps) ~ (Private + log(Top10perc) + (Top25perc)^2 +       
                           log(F.Undergrad) +log(P.Undergrad) +  Outstate + 
                           (Room.Board) + log(Books) + log(Personal) +         
                           log(PhD)+Terminal+log(S.F.Ratio)+ perc.alumni +    
                           log(Expend) + Grad.Rate+ Elite)^2, 
                           data=College.train)

#our best linear model using BIC
app_best = step(app_lm_t2, k=log(nrow(College.train)))

summary(app_best)
#diagnostic plots
plot(app_best,labels.id = College.train$college)

# Termplots for further diagnostics
par(mfrow=c(2,2))
termplot(app_best, data=College.train,partial.resid=T, smooth=panel.smooth, se=T)
```



6.  Repeat problem 4, but using your model from problem 5.  If you transform the response, you will need to back transform  data to the original units in order to compute the RMSE in the original units.  Does this suggest that the model is adequate?  Do the two graphs provide information about which model is better?


```{r}

nsim=1000
rmse = function(y, ypred){
  rmse=sqrt(mean((y-ypred)^2))
  return(rmse)
}

X<-model.matrix(log(Apps) ~ Private + log(Top10perc) + Top25perc + 
                                       log(F.Undergrad) + log(P.Undergrad) + Outstate + 
                                       Room.Board + log(PhD) + log(S.F.Ratio) + 
                                       log(Expend) + Grad.Rate + Private:Room.Board + 
                                       log(Top10perc):Top25perc + log(Top10perc):log(Expend) +  
                                       log(P.Undergrad):Outstate + Outstate:log(Expend) + 
                                       Room.Board:log(Expend) + log(S.F.Ratio):Grad.Rate + 
                                       log(Expend):Grad.Rate, 
                                       data = College.train)
params<-sim(app_best, nsim)
Y<-matrix(0, nrow=nrow(College.train),ncol=nsim )
RMSE.2<-rep(0,nsim)
# note that we had log-transformed the response; hence we need to exponentiate the simulated responses
for(j in 1:nsim){
Y[,j]<-exp(X %*% params@coef[j,] + rnorm(1, 0, params@sigma[j]))
RMSE.2[j]<-rmse(Y[,j],exp(predict(app_best)))
  
}
RMSE.2<-as.data.frame(RMSE.2)


RMSE.1<-rmse(College.train$Apps,exp(predict(app_best)))
GGPLOT.6<-ggplot(data= RMSE.2, aes(x=RMSE.2))+geom_histogram() + geom_vline(xintercept = RMSE.1, col="red")+labs(x="RMSE", y="Count")
print(GGPLOT.6)

pval<-sum(RMSE.2>RMSE.1)/nsim
print(pval)

```



7. Use your two fitted models to predict the number of applications for the testing data, `College.test`.  Plot the predicted residuals $y_i - \hat{y}_i$  versus the predictions.  Are there any cases where the model does a poor job of predicting?  Compute the RMSE using the test data.

ANSWER: We find that the observed value is closer to the mode of the simulated RMSE distribution. As indicated by the current p-value of 'r pval', the current model is a better fit to the observed data in the training set. Of course, this is based on a single summary statistic only (RMSE), and cannot be used for final approval of the model.

```{r}

nsim=1000
rmse = function(y, ypred){
  rmse=sqrt(mean((y-ypred)^2))
  return(rmse)
}

X<-model.matrix(log(Apps) ~ Private + Top25perc + log(F.Undergrad) + 
    log(P.Undergrad) + Outstate + Room.Board + log(Books) + log(Personal) + 
    log(PhD) + log(Expend) + Grad.Rate + Private:Room.Board + 
    Top25perc:log(Books) + log(P.Undergrad):Outstate + Outstate:log(Expend) + 
    Room.Board:log(Expend) + log(Books):log(Personal) + log(Books):Grad.Rate + 
    log(PhD):Grad.Rate, data=College.train)
params<-sim(apps.best, nsim)
Y<-matrix(0, nrow=nrow(College.train),ncol=nsim )
RMSE.2<-rep(0,nsim)
# note that we had log-transformed the response; hence we need to exponentiate the simulated responses
for(j in 1:nsim){
Y[,j]<-exp(X %*% params@coef[j,] + rnorm(1, 0, params@sigma[j]))
RMSE.2[j]<-rmse(Y[,j],exp(predict(apps.best)))
  
}
RMSE.2<-as.data.frame(RMSE.2)


RMSE.1<-rmse(College.train$Apps,exp(predict(apps.best)))
GGPLOT.best<-ggplot(data= RMSE.2, aes(x=RMSE.2))+geom_histogram() + geom_vline(xintercept = RMSE.1, col="red")
print(GGPLOT.best)

pval<-sum(RMSE.2>RMSE.1)/nsim
print(pval)

```


7. Use your two fitted models to predict the number of applications for the testing data, `College.test`.  Plot the predicted residuals $y_i - \hat{y}_i$  versus the predictions.  Are there any cases where the model does a poor job of predicting?  Compute the RMSE using the test data where now RMSE = $\sqrt{\sum_{i = 1}^{n.test}(y_i - \hat{y}_i)^2/n.test}$ where the sum is over the test data.  Which model is better for the out of sample prediction?


ANSWER: First we plot the predictions for the base model with main effects only, and the 'best' model obtained by BIC. We found that a few predictions have an excessively high residual, so we remove those from the plot.

```{r}
# rerun the base model
apps.lm0<-lm(Apps~., data=College.train[,-17])
a.new<-predict(apps.lm0, newdata = College.test[,-17])
b.new<-predict(app_best, newdata=College.test)

# remove those with residual > 8000
ind.a<-which(abs(a.new-College.test$Apps)>8000)
ind.b<-which(abs(exp(b.new)-College.test$Apps)>8000)

par(mfrow=c(1,2))
plot(a.new[-ind.a],a.new[-ind.a]-College.test$Apps[-ind.a],xlab="Predicted", ylab="Residual",main ="Base model (#4)")
plot(exp(b.new[-ind.b]), exp(b.new[-ind.b])-College.test$Apps[-ind.b],,xlab="Predicted", ylab="Residual",main="Best model (#6)")

print(College$college[ind.a])
print(College$college[ind.b])


```
The wors fits (residual >8000) are for  "Antioch University"  and  "College of Saint Catherine" for the base model, and "College of Saint Catherine" and "Dordt College" for the 'best' linear  model. Next, we compute the RMSE of the test data.

```{r}
RMSE.0.test<-sqrt(mean((a.new-College.test$Apps)^2))
RMSE.best.test<-sqrt(mean((exp(b.new)-(College.test$Apps))^2))

print(RMSE.0.test)
print(RMSE.best.test)
```

With an RMSE of 'r RMSE.best' the out of sample prediction is better for the model with interactions and transformations compared to the base model which has an RMSE of 'r RMSE.0'.


8. Add the test RMSE's from the two models to the respective  histograms from 4 and 6.   Are these values surprising relative to the RMSEs from the replicated data?  Explain.  What do you think this implies for model adequacy checks?  How accurately can we predict college applications?


ANSWER: If the model was true, then yes, these values would be surprising. 
```{r}
par(mfrow=c(1,2))
plot(GGPLOT.4+geom_vline(xintercept = RMSE.0.test, col="yellow"))
plot(GGPLOT.6+ geom_vline(xintercept = RMSE.best.test, col="yellow"))

```

9.  As the number of applications is a count variable, a Poisson regression model is a natural alternative for modelling this data.   Build a Poisson model using only main effects as in problem 4.   Comment on the model adequacy based on diagnostic plots and other summaries.  Is there evidence that there is lack of fit?

```{r}
Apps.poi = glm(Apps ~ .  -college, family = poisson(link = "log"), data = College.train)
summary(Apps.poi)
par(mfrow=c(2,2))
plot(Apps.poi)

#checking for overdispersion
Apps.poi$deviance/Apps.poi$df.residual
```
ANSWER: Due to the inflexible variance structure (variance=mean) of the Poisson, we find that all predictors are highly significant. The residual deviance is very large, and the ratio of residual deviance divided by the residual degrees of freedom is 683 which is $\qq 1$, indicating substantial overdispersion (no need for a formal test here!). In the diagnostic plots, we observe that  some of the standard deviance residuals are huge - indicating cause for concern. In summary, the overdispersion indicates a lack of fit.

10.  Generate 1000 replicates data sets using the coefficients from the Poisson model you fit above.  Using RMSE as a statistic, $\sqrt{\sum_i(y^{\text{rep}} - \hat{y}_i^{\text{rep}})^2/n }$, how does the RMSE from the model based on the training data compare to RMSE's based on the replicated data.  What does this suggest about model adequacy?   Provide a histogram of the RMSE's with a line showing the location of the observed RMSE and compute a p-value.

ANSWER: The RMSE of the model is completely out of whack compared to the replicated data (p value is estiamted to be 0 based on 1000 simulations). In conculsion, based on the RMSE as a test statistic, we have strong evidence to reject the Poisson model.

```{r}
S = 1000
beta = sim(Apps.poi, n.sims = 1000)
X = model.matrix(Apps.poi)
Y = matrix(NA, nrow(College.train), ncol = S)

rmse.train.poi = rmse(predict(Apps.poi), College.train$Apps)
rmse.rep.poi = numeric(S)

for (s in 1:S) {
  lambda = exp(X %*% beta@coef[s,]) 
  Y[, s] = rpois(nrow(College.train),lambda)
  rmse.rep.poi[s] = rmse(predict(Apps.poi), Y[, s])
}


rmse.rep.poi = as.data.frame(rmse.rep.poi)

hist.poi <-ggplot(data= rmse.rep.poi, aes(x=rmse.rep.poi)) + geom_histogram(fill = "slateblue") + geom_vline(xintercept = rmse.train.poi, col="red")
print(hist.poi)

pval <-sum(rmse.rep.poi > rmse.train.poi)/S
print(pval)
```


11.  Using the test data set, calculate the RMSE for the test data using the predictions from the Poisson model.  How does this compare to the RMSE based on the observed data?  Add this RMSE to your plot above.

ANSWER: Below, we show the simulated RMSE, together with the training RMSE (red) and the test RMSE (blue) -- we see that the latter is much larger than the training RMSE (not surprising).

```{r}
Apps.poi = glm(Apps ~ . , family = poisson(link = "log"), data = College.train[,-17])
predict(Apps.poi, newdata=College.test[,-17])

rmse.test = rmse(College.test$Apps, predict(Apps.poi, newdata=College.test[,-17]))
hist.poi <- hist.poi + geom_vline(xintercept = rmse.test, colour = "cornflowerblue")
plot(hist.poi)
```

12.  As in problem 6,  consider transformations of the predictors and interactions to improve your model justifying your choices.  Provide a summary of the model and comment on diagnostics and summaries for model fit.   Is there evidence of overdispersion?  Explain.

ANSWER: Given the overdispersion, a first step is to add a variable dispersion parameter, i.e., use a quasi-Poisson model. Of course, this will not change the deviance as the likelihood remains the same, but it will increase the standard errors and give a better idea of which predictors are important.

```{r}
# Quasi-Poisson 
Apps.qpoi <- glm(Apps ~ . - college, family = quasipoisson(link = "log"), data = College.train)
summary(Apps.qpoi)
par(mfrow=c(2,2))
plot(Apps.qpoi)
```


Looking at the diagnostic plots, we find that using the overdispersed Poisson helped substantially. It brought the standard deviance residual from a max of ~10 to below 2, and a max Std Pearson residual from 150 to less than 5.  There is still a trend in the Scale-Location plot. Next, we used the predictor transformations and interactions from the best linear model to see if this would resolve some of these issues (note we cannot use the step procedure for the QP because there is no explicit likelihood for this model). As shown below, we found that the pattern in the residual plot is attenuated, and the trend in the std deviance residual plot is lower than in the previous model. In addition, there is 


```{r}
# Quasi-Poisson with transformations and interactions
Apps.qpoi <- glm(Apps ~ Private + log(Top10perc) + log(Top25perc) + 
    log(F.Undergrad) + log(P.Undergrad) + Outstate + Room.Board + 
    log(PhD) + log(S.F.Ratio) + log(Expend) + log(Grad.Rate) + 
    Private:Room.Board + log(Top10perc):log(Top25perc) + log(P.Undergrad):Outstate + 
    Outstate:log(Expend) + Room.Board:log(Expend) + log(S.F.Ratio):log(Grad.Rate) + 
    log(Expend):log(Grad.Rate), family = quasipoisson(link = "log"), data = College.train)

Apps.qpoi$deviance/Apps.qpoi$df.residual
pchisq(Apps.qpoi$deviance, Apps.qpoi$df.residual, lower.tail = F)
```
While the quassipoisson ostensibly resulted in an improved outcome, considering that it brought the standard deviance residuals way down, its estimated overdispersion is still very high, and the chi-square test indicates that this model is not a good fit either.

13. Carry out the predictive check using simulated data from the Poisson model.   Add the RMSE for the observed data to the plot and the RMSE for prediction on the test data.  Does this suggest that the model is OK?

```{r}
S = 1000
beta.q = sim(Apps.qpoi, n.sims = 1000)
X.q = model.matrix(Apps ~ . -Apps -college, data = College.test)
Y.q = matrix(NA, nrow(College.test), ncol = S)

rmse.test.qpoi = rmse(predict(Apps.qpoi), College.test$Apps)
rmse.rep.qpoi = rep(0, S)

for (s in 1:S) {
  Y.q[, s] = exp(X.q %*% beta.q@coef[s,]) + rnorm(1, 0, beta.q@sigma[s])
  rmse.rep.qpoi[s] = rmse(predict(Apps.qpoi), Y[, s] )
}

rmse.q.df = data.frame(rsme.test.qpoi = rmse.test.qpoi, rmse.rep.qpoi = rmse.rep.qpoi)
rmse.q.df
rmse.rep.qpoi = as.data.frame(rmse.rep.qpoi)

hist.qpoi <-ggplot(data= rmse.rep.qpoi, aes(x=rmse.rep.qpoi)) + geom_histogram(fill = "slateblue") + geom_vline(xintercept = rmse.test.qpoi, col="red")
print(hist.qpoi)

pval <-sum(rmse.rep.qpoi > rmse.test.qpoi)/S
print(pval)
```



14. Build a model using the negative binomial model (consider transformations and interactions if needed) and examine diagnostic plots.  Are there any suggestions of problems with this model?

```{r}
Apps.nb <- glm.nb(Apps ~ . -Apps -college, data = College, link = log)
summary(Apps.nb)
plot(Apps.nb)

Apps.nb$deviance/Apps.nb$df.residual
pchisq(Apps.nb$deviance, Apps.nb$df.residual, lower.tail = F)
```
With the inital model, there is somewhat of a trend in the residual plot, indicating a less than adequate model fit. 

15. Carry out the predictive checks using simulated replicates with RMSE and add RMSE from the test data and observed data to your plot.  What do these suggest about 1) model adequacy and 2) model comparison?  Which model out of all that you have fit do you recommend?

```{r}
S = 1000
beta.nb = sim.negbin(Apps.nb, n.sims = 1000)
X.nb = model.matrix(Apps ~ . -Apps -college, data = College.test)
Y.nb = matrix(NA, nrow(College.test), ncol = S)

rmse.test.nb = rmse(predict(Apps.nb), College.test$Apps)
rmse.rep.nb = rep(0, S)

for (s in 1:S) {
  Y.nb[, s] = exp(X.nb %*% beta.nb@coef[s,]) + rnorm(1, 0, beta.nb@sigma[s])
  rmse.rep.nbp[s] = rmse(predict(Apps.nb), Y[, s] )
}

rmse.nb.df = data.frame(rsme.test.nb = rmse.test.nb, rmse.rep.nb = rmse.rep.nb)
rmse.q.df
rmse.rep.nb = as.data.frame(rmse.rep.nb)

hist.nb <-ggplot(data= rmse.rep.nb, aes(x=rmse.rep.nb)) + geom_histogram(fill = "slateblue") + geom_vline(xintercept = rmse.test.nb, col="red")
print(hist.nb)

pval <-sum(rmse.rep.nb > rmse.test.nb)/S
print(pval)
```


16.  While RMSE is a popular summary for model goodness of fit, coverage of confidence intervals is an alternative.
For each case in the test set, find a 95% prediction interval.  Now evaluate if the responses in the test data are inside or outside of the intervals.   If we have the correct coverage, we would expect that at least 95\% of the intervals would contain the test cases.
Write a function to calculate coverage (the input should be the fitted model object and the test data-frame) and then evaluate coverage for each of the 5 models that you fit  (the two normal, the two Poisson and the negative binomial) including plots of the confidence intervals versus ordered by the prediction, with the left out data added as points.  Comment on the plots, highlighting any unusual colleges where the model predicts poorly.

17.  Provide a table  with 
the 1) RMSE's on the observed data, 2) RMSE's on the test data, 3) coverage, 4) the predictive check p-value with one row for each of the 5 models and comment the results.  Which model do you think is best and why? 

18.  For your "best" model  provide a nicely formatted table (use `kable()` or `xtable()`) of relative risks and 95% confidence intervals.  Pick 5 of the most important variables and provide a paragraph that provides an interpretation of the parameters (and intervals) that can be provided to a university admissions officer about which variables increase admissions.  

### Some Theory   

19.   We have said that the residual deviance has a $\chi^2$ distribution with $n - p - 1$  degrees of freedom (n - the number of parameters) if the model is correct.   What is the mean and standard deviation of the residual deviance?    Use the Empirical Rule to suggest an easy to use cut-off for suggesting overdispersion that you can calculate without a calculator.

ANSWER: The residual deviance has mean  $n-p-1$ and standard deviation $\sqrt{2(n-p-1)}$ (basic results from distribution theory). By definition of a $\Chi^2$, the residual deviance is equal in distribution to $\sum_{i=1}^{n-p-1} Z_i^2$ where $Z_i$ are i.i.d. standard normals. By virtue of the CLT, we have for large residual degrees of freedom $df=n-p-1$,
$$ \sqrt{n-p-1} \left( \frac{\sum_{i=1}^{n-p-1} Z_i^2}{n-p-1}-1\right) \approx \mathcal{N}(0, 2).$$
Therefore, we have that the residual deviance $D$ divided by $df$ satisfies
$$\frac{D}{df}-1 \approx \mathcal{N}(0, \frac{2}{df})$$
Using the Empirical Rule (95% confidence interval approximated by $\pm 2\sigma$ for normal), we derive a heuristic cutoff of 
$$ \frac{D}{f}> 1+ 2 \sqrt{\frac{2}{df}}=1+ \sqrt{\frac{8}{df}}.$$



20.  Gamma mixtures of Poissons:  From class we said that
\begin{align}
Y \mid \lambda & \sim P(\lambda) \\
p(y \mid \lambda) & = \frac{\lambda^y e^{-\lambda}} {y!} \\
& \\
\lambda \mid \mu, \theta & \sim G(\theta, \theta/\mu)  \\
p(\lambda \mid  \mu, \theta) & = \frac{(\theta/ \mu)^\theta}{\Gamma(\theta)} \lambda^{\theta - 1} e^{- \lambda \theta/\mu} \\
& \\
p(Y \mid \mu, \theta) & = \int p(Y \mid \lambda) p(\lambda \mid \theta, \theta/\mu) d \lambda \\
 & =   \frac{ \Gamma(y + \theta)}{y! \Gamma(\theta)}
\left(\frac{\theta}{\theta + \mu}\right)^{\theta}
\left(\frac{\mu}{\theta + \mu}\right)^{y} \\
Y \mid \mu, \theta & \sim NB(\mu, \theta) 
\end{align}
Derive the density of $Y \mid \mu, \theta$ in (8) showing your work using LaTeX expressions.  (Note this may not display if the output format is html, so please use pdf.)
Using iterated expectations with the Gamma-Poisson mixture, find the mean and variance of $Y$, showing your work.
\begin{align}
p(Y \mid \mu, \theta) & = \int p(Y \mid \lambda) p(\lambda \mid \theta, \theta/\mu) d \lambda \\
& = \int \frac{\lambda^y e^{-\lambda}} {y!}\frac{(\theta/ \mu)^\theta}{\Gamma(\theta)} \lambda^{\theta - 1} e^{- \lambda \theta/\mu} d \lambda \\
& = \frac{(\theta/ \mu)^\theta}{\Gamma(\theta)y!} \int\lambda^{y+\theta - 1} e^{- \lambda (\theta/\mu+1)} d \lambda \\
& = \frac{(\theta/ \mu)^\theta}{\Gamma(\theta)y!} \frac{\Gamma(y+\theta)}{(1+\theta / \mu)^{\theta+y}} \\
& =  \frac{ \Gamma(y + \theta)}{y! \Gamma(\theta)}
\left(\frac{\theta}{\theta + \mu}\right)^{\theta}
\left(\frac{\mu}{\theta + \mu}\right)^{y} \\
E(Y) &= E[E(Y \mid \lambda)] \\
&= E[\lambda] \\
&= \frac{\theta}{\theta / \mu} \\
&= \mu \\
Var(Y) &= E[Var(Y \mid \lambda)] + Var[E(Y \mid \lambda)] \\
&= E(\lambda) + Var(\lambda)\\
& = \mu + \frac{\theta}{\theta^2 / \mu^2}\\
& = \mu + \frac{\mu^2}{\theta}
\end{align}

